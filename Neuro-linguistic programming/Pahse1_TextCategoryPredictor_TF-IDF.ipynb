{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of NLP981_Phase1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arman76/hello-world/blob/master/NLP981_Phase1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Tduc6QDQz1H",
        "colab_type": "text"
      },
      "source": [
        "# NLP981 Final Project - Phase #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0F0hIUs7oCS",
        "colab_type": "text"
      },
      "source": [
        "*   Instructor: Javad PourMostafa\n",
        "*   Teaching Assistant: Parsa Abbasi\n",
        "*   University of Guilan, 1st semester of 2019\n",
        "*   GitHub repository : *https://github.com/JoyeBright/NLP*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVC1mwiaOZMI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install stopwords_guilannlp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUO9K1mKO2EB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from itertools import chain\n",
        "from collections import Counter\n",
        "import re\n",
        "import math\n",
        "import stopwords_guilannlp\n",
        "import numpy as np\n",
        "\n",
        "STOP_WORDS = stopwords_guilannlp.stopwords_output('Persian', 'list')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aE9xQhhQ6XS",
        "colab_type": "text"
      },
      "source": [
        "It's the first phase of your final project for the *NLP981* course. The main idea behind this phase is to portray the develope side of *NLP*.\n",
        "\n",
        "You must code inside of this python notebook. I highly recommend you to use the *Google Colab* environment. \n",
        "\n",
        "If you have any questions, feel free to ask.\n",
        "You can use [*Quera*](https://quera.ir/course/4385/) platform for your general questions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgS3aCY358dV",
        "colab_type": "text"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FG4cndz6DDq",
        "colab_type": "text"
      },
      "source": [
        "A category predictor is going to build at this phase of the project.\n",
        "\n",
        "The predictor gets a text as input and predicts a category for that.\n",
        "\n",
        "For this purpose, you need to :\n",
        "\n",
        "1.   Load the dataset\n",
        "2.   Preprocess the text data\n",
        "3.   Implement a word representation method to represent each text as a numeric vector\n",
        "4.   Implement a classification model and train that using the training set\n",
        "5.   Predict a category for each of validation data using implemented model\n",
        "6.   Measure your work using confusion matrix and some common metrics\n",
        "\n",
        "**Important Note:** You can use any library you want in sections 1 and 2. But everything in section 3-6 need to be coded purely.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0acil66KQ8A_",
        "colab_type": "text"
      },
      "source": [
        "## 1) Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haH1kfj3Q9tf",
        "colab_type": "text"
      },
      "source": [
        "The dataset you will use in this phase is called *Divar* that released by the *CafeBazaar* research team.\n",
        "\n",
        "It contains more than 900,000 posts of the *Divar* ads platform. We split this dataset into training, validation, and testing sets.\n",
        "\n",
        "The testing set is not accessible for you, and we use them to evaluate your work on the presentation day.\n",
        "\n",
        "You can download the dataset files (training and validation sets) directly from the following link :\n",
        "\n",
        "> *https://drive.google.com/open?id=1oj-fqpymjDr8QsOK-zQliiqXbVqakrFo*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdIRg1UBSOEi",
        "colab_type": "text"
      },
      "source": [
        "### 1.1) Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwCPJjaSQukm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the training and validation sets here\n",
        "train_set = pd.read_csv('./drive/My Drive/trainset.csv')\n",
        "train_set = train_set.drop(['Unnamed: 0', 'Unnamed: 0.1', 'archive_by_user',\n",
        "                            'brand', 'cat2', 'cat3', 'city', 'created_at', 'id',\n",
        "                            'image_count', 'mileage', 'platform', 'price', 'type',\n",
        "                            'desc', 'year'], axis=1)\n",
        "\n",
        "valid_set = pd.read_csv('./drive/My Drive/validationset.csv')\n",
        "valid_set = valid_set.drop(['Unnamed: 0', 'Unnamed: 0.1', 'archive_by_user',\n",
        "                            'brand', 'cat2', 'cat3', 'city', 'created_at', 'id',\n",
        "                            'image_count', 'mileage', 'platform', 'price', 'type',\n",
        "                            'desc', 'year'], axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6g4085BgwEyv",
        "colab_type": "code",
        "outputId": "a60183a5-2761-4f89-e076-86c46eb5f2f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWHwVPkfS-OX",
        "colab_type": "text"
      },
      "source": [
        "### 1.2) Analyzing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlIKCffqU4AM",
        "colab_type": "text"
      },
      "source": [
        "Display the top 10 rows of the train set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw0EuzrFVIat",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "78afeb85-5e9b-4475-bb89-d27ea6ba6888"
      },
      "source": [
        "print(train_set.head(n=10))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                 cat1                                     title\n",
            "0            personal                                   سایز 40\n",
            "1  electronic-devices                      گوشی سامسونگ a3 2016\n",
            "2            personal                     ساعت زنانه اسپریت اصل\n",
            "3            personal  فروش یک عدد دوچرخه مارک پرادو بسیار سالم\n",
            "4     leisure-hobbies                  14 اسکناس کلکسیونی code1\n",
            "5     leisure-hobbies                       فوتبال دستی حرفه ای\n",
            "6        for-the-home                                     لوستر\n",
            "7            vehicles                           رینگ اهنی ال 90\n",
            "8            vehicles                        نیسان سرانزا مدل۸۳\n",
            "9            vehicles                                پراید سفید\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqfkJST6VMXt",
        "colab_type": "text"
      },
      "source": [
        "How many data (rows) stored in the training and validation sets?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jkiaz92zVVpy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_len = len(train_set)\n",
        "valid_len = len(valid_set)\n",
        "train_set = train_set[:3500]\n",
        "valid_set = valid_set[:500]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aw0x_s3_b4U",
        "colab_type": "text"
      },
      "source": [
        "How many posts are in each category (First level categories)? (Based on training set)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31PFgy46_ntw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "22761ce1-2172-4fa5-bf04-91a588e56dd8"
      },
      "source": [
        "cats_counts = Counter(chain.from_iterable([[i] for i in train_set.cat1]))\n",
        "print(cats_counts)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({'for-the-home': 1057, 'vehicles': 767, 'electronic-devices': 606, 'personal': 504, 'leisure-hobbies': 343, 'businesses': 223})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUejNXljlZD1",
        "colab_type": "text"
      },
      "source": [
        "## 2) Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-j1EilpbAi-W",
        "colab_type": "text"
      },
      "source": [
        "There are two kinds of text data in the dataset: *Title* and *Description*.\n",
        "You can use one or both of them as text inputs of your classification model. Choose a composition that gives you a higher measuring score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLT50bemjw3K",
        "colab_type": "text"
      },
      "source": [
        "You need to apply some preprocessing procedures on your text data first. We want at least **4** preprocessing step from you. It can be removing stop words, removing punctation, removing or replacing digits, stemming, lemmatizing, normalization, and so on.\n",
        "\n",
        "You can use the [*Stopwords Guilan NLP*](https://github.com/JoyeBright/stopwords_guilannlp) library to access a collection of Persian stop words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPQYAeEnlrcc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessing(text: str):\n",
        "    text = text.replace('\\u200c', ' ')\n",
        "    text = text.replace('\\t', ' ')\n",
        "    text = text.replace('$NUM', ' ')\n",
        "    text = re.sub(re.compile('[/(){}\\[\\]\\|@,;]'), ' ', text, )\n",
        "    text = re.sub(re.compile('[✔️۰۱۲۳⚡۴۵\\-۶۷=۸❌۹؛0%-9*.⤵٢٠١٥❤?&٩$#+!٦٨\"،٧٤؟●٣]'), ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    cleared_text = ' '.join([i for i in text.split() if i not in STOP_WORDS])\n",
        "\n",
        "    return cleared_text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhZwgvwCH8rs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set['title'] = [preprocessing(t) for t in train_set.title]\n",
        "valid_set['title'] = [preprocessing(t) for t in valid_set.title]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izN13hPulonI",
        "colab_type": "text"
      },
      "source": [
        "## 3) Word Representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeC-rd4DMSUb",
        "colab_type": "text"
      },
      "source": [
        "As you know, classification models can't deal with strings directly, and you have to represent your texts in a numerical form."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1rGMFJmqKwm",
        "colab_type": "text"
      },
      "source": [
        "### 3.1) Tf-idf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfPOmAF6qOF0",
        "colab_type": "text"
      },
      "source": [
        "You have to implement the tf-idf vectorization method from scratch in this step. \n",
        "\n",
        "Furthermore, a function must be implemented that gives a text input and return a tf-idf vectorized representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTZCMdqft4Ic",
        "colab_type": "text"
      },
      "source": [
        "$$\\text{tf-idf}(t, d) = \\text{tf}(t, d) \\times \\text{idf}(t)$$\n",
        "\n",
        "*tf* (term-frequency) is the count of occurrences of the word `t` in specific text `d`.\n",
        "\n",
        "*idf* (inverse document-frequency) is term that is inversely proportional to the number of texts with the given word. It can be calculated this way:\n",
        "$$\\text{idf}(t) = \\text{log}\\frac{1 + n_d}{1 + n_{d(t)}} + 1$$\n",
        "where $n_d$ is the whole number of texts and $n_{d(t)}$ is the number of texts with the word `t`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVwNZTGLqm20",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def doc_dict(BofW):\n",
        "    d = dict.fromkeys(unique_Words, 0)\n",
        "    for word in BofW:\n",
        "        try:\n",
        "            d[word] += 1\n",
        "        except:\n",
        "            pass\n",
        "    return d\n",
        "\n",
        "\n",
        "def tf_computation(document, bag_of_words):\n",
        "    tf_doc = {}\n",
        "    bow_count = len(bag_of_words)\n",
        "    # print(bow_count)\n",
        "    for w, count in document.items():\n",
        "        tf_doc[w] = float(count / bow_count)\n",
        "    return tf_doc\n",
        "\n",
        "\n",
        "def idf_computation(docs):\n",
        "    n = len(docs)\n",
        "    idf_dict = dict.fromkeys(docs[0].keys(), 0)\n",
        "    for document in docs:\n",
        "        for w, val in document.items():\n",
        "            if val > 0:\n",
        "                idf_dict[w] += 1\n",
        "    for w, val in idf_dict.items():\n",
        "        idf_dict[w] = math.log((n + 1)/float(val + 1)) + 1\n",
        "    return idf_dict\n",
        "\n",
        "def tf_idf_computation(tf, idfs):\n",
        "    tf_idf = {}\n",
        "    for w, val in tf.items():\n",
        "        tf_idf[w] = val * idfs[w]\n",
        "    return tf_idf\n",
        "\n",
        "def tf_idf(text):\n",
        "    text_BoW = text.split(' ')\n",
        "\n",
        "    doc = doc_dict(text_BoW)\n",
        "    tf_doc = tf_computation(doc, text_BoW)\n",
        "    vector = tf_idf_computation(tf_doc, idf_s)\n",
        "    return vector\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTu-5G6vRuK6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unique_Words = dict()\n",
        "idf_s = dict()\n",
        "n = 0\n",
        "def fit_transform(X):\n",
        "    global unique_Words\n",
        "    global idf_s\n",
        "    global n\n",
        "    BoW = []\n",
        "    unique_Words = dict()\n",
        "    for row in X:\n",
        "        BoW.append(row.split(' '))\n",
        "        unique_Words = set(unique_Words).union(set(BoW[-1]))\n",
        "\n",
        "    unique_Words = sorted(unique_Words)\n",
        "    print('length of unique words: ' + str(len(unique_Words)))\n",
        "\n",
        "    Docs = [doc_dict(x) for x in BoW]\n",
        "    n = len(Docs)\n",
        "    idf_s = idf_computation(Docs)\n",
        "\n",
        "    return pd.DataFrame([tf_idf(x) for x in X])\n",
        "\n",
        "def transform(X):\n",
        "    global n\n",
        "    global idf_s\n",
        "\n",
        "    Bow = []\n",
        "    for row in X:\n",
        "        Bow.append(row.split(' '))\n",
        "        for word in Bow[-1]:\n",
        "            if word not in idf_s:\n",
        "                idf_s[word] = math.log((n + 1)/float(0 + 1)) + 1\n",
        "    Docs = [doc_dict(x) for x in Bow]\n",
        "    \n",
        "    return pd.DataFrame([tf_idf(x) for x in X])\n",
        "\n",
        "def labels(x):\n",
        "    if x == 'vehicles' or x == 0:\n",
        "        return 0\n",
        "    elif x == 'for-the-home' or x == 1:\n",
        "        return 1\n",
        "    elif x == 'electronic-devices' or x == 2:\n",
        "        return 2\n",
        "    elif x == 'leisure-hobbies' or x == 3:\n",
        "        return 3\n",
        "    elif x == 'businesses' or x == 4:\n",
        "        return 4\n",
        "    else:\n",
        "        return 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YXB3Aa3Zb03",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5a1689c7-6e5f-410f-bd9f-47ffcbf71b64"
      },
      "source": [
        "tf_idf_docs = fit_transform(train_set.title)\n",
        "tf_idf2 = transform(valid_set.title)"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "length of unique words: 3038\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSDMXv46CWwm",
        "colab_type": "text"
      },
      "source": [
        "## 4) Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRpciSZZ-mb9",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://cdn.lynda.com/course/578082/578082-637075371482276339-16x9.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk1EJq7B4JwX",
        "colab_type": "text"
      },
      "source": [
        "### 4.1) Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoiyVNNz-q5k",
        "colab_type": "text"
      },
      "source": [
        "The Logistic Regression classifier must be implemented from scratch here.\n",
        "\n",
        "You can fit the training data into the classifier after implementing linear regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5q-s73zI-xOq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "\n",
        "def cost(theta, x, y):\n",
        "    h = sigmoid(x @ theta)\n",
        "    m = len(y)\n",
        "    cost = 1 / m * np.sum(\n",
        "        -y * np.log(h) - (1 - y) * np.log(1 - h)\n",
        "    )\n",
        "    grad = 1 / m * ((y - h) @ x)\n",
        "    return cost, grad\n",
        "\n",
        "\n",
        "def fit(x, y, max_iter=5000, alpha=0.1):\n",
        "    x = np.array(x)\n",
        "    x = np.insert(x, 0, 1, axis=1)\n",
        "    thetas = []\n",
        "    classes = np.unique(y)\n",
        "    costs = np.zeros(max_iter)\n",
        "\n",
        "    for c in classes:\n",
        "        binary_y = np.where(y == c, 1, 0)\n",
        "\n",
        "        theta = np.zeros(x.shape[1])\n",
        "        for epoch in range(max_iter):\n",
        "            costs[epoch], grad = cost(theta, x, binary_y)\n",
        "            theta += alpha * grad\n",
        "\n",
        "        thetas.append(theta)\n",
        "    return thetas, classes, costs\n",
        "\n",
        "\n",
        "def predict(classes, thetas, x):\n",
        "    x = np.array(x)\n",
        "    x = np.insert(x, 0, 1, axis=1)\n",
        "    preds = [np.argmax(\n",
        "        [sigmoid(xi @ theta) for theta in thetas]\n",
        "    ) for xi in x]\n",
        "    return [classes[p] for p in preds]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5M6wT_dwkPh",
        "colab_type": "code",
        "outputId": "53e13c74-99ee-4f9c-dec2-6eb2a65d0d9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y = [labels(x) for x in train_set.cat1]\n",
        "valid_y = [labels(x) for x in valid_set.cat1]\n",
        "print(y[:20])\n",
        "thetas, classes, costs = fit(tf_idf_docs, y)\n"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5, 2, 5, 5, 3, 3, 1, 0, 0, 0, 3, 2, 4, 1, 2, 3, 5, 1, 2, 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k089_muZ6j-j",
        "colab_type": "text"
      },
      "source": [
        "## 5) Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kc5dl49O3aST",
        "colab_type": "text"
      },
      "source": [
        "Now you can predict a category for each of the validation data using the implemented classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bM9Shkx3x7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ilabels(x):\n",
        "    if x == 'vehicles' or x == 0:\n",
        "        return 'vehicles'\n",
        "    elif x == 'for-the-home' or x == 1:\n",
        "        return 'for-the-home'\n",
        "    elif x == 'electronic-devices' or x == 2:\n",
        "        return 'electronic-devices'\n",
        "    elif x == 'leisure-hobbies' or x == 3:\n",
        "        return 'leisure-hobbies'\n",
        "    elif x == 'businesses' or x == 4:\n",
        "        return 'businesses'\n",
        "    else:\n",
        "        return 'personal'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_hclzRuoTfY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "outputId": "70774711-468b-4692-a099-88c083b38fc2"
      },
      "source": [
        "y_p_train = predict(classes, thetas, tf_idf_docs)\n",
        "print('valid set: ')\n",
        "yp = predict(classes, thetas, tf_idf2)\n",
        "print(yp)\n",
        "a = ['هندزفری', 'مبل', 'کفش', 'گوشی الجی']\n",
        "ps = predict(classes, thetas, transfom(a))\n",
        "for i in range(len(a)):\n",
        "    print(a[i], '\\t\\t:', ilabels(ps[i]))"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "valid set: \n",
            "[0, 0, 0, 1, 5, 4, 5, 2, 1, 2, 0, 0, 1, 1, 5, 0, 1, 1, 0, 1, 3, 1, 3, 1, 1, 3, 4, 3, 0, 1, 5, 0, 0, 2, 5, 2, 0, 1, 0, 1, 3, 1, 2, 2, 4, 5, 5, 4, 5, 3, 0, 1, 1, 1, 5, 0, 1, 1, 0, 1, 1, 1, 2, 0, 1, 5, 5, 2, 2, 0, 2, 1, 1, 5, 1, 0, 5, 2, 1, 1, 1, 2, 5, 5, 1, 5, 4, 3, 0, 1, 1, 3, 0, 1, 1, 2, 1, 0, 1, 5, 0, 0, 2, 5, 5, 1, 0, 4, 1, 2, 2, 1, 5, 1, 0, 0, 1, 0, 1, 2, 1, 1, 2, 5, 1, 2, 2, 4, 2, 3, 1, 5, 1, 1, 2, 2, 1, 2, 5, 0, 1, 1, 1, 3, 1, 1, 2, 4, 0, 1, 3, 2, 5, 5, 2, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 3, 2, 1, 1, 1, 0, 1, 1, 1, 1, 5, 0, 2, 2, 1, 1, 1, 5, 1, 0, 1, 1, 0, 1, 5, 1, 5, 1, 1, 1, 2, 4, 3, 1, 0, 3, 5, 1, 1, 0, 0, 1, 5, 5, 1, 1, 1, 0, 0, 1, 1, 2, 0, 1, 1, 1, 0, 1, 0, 0, 1, 5, 0, 2, 0, 1, 0, 1, 5, 2, 0, 5, 2, 1, 1, 3, 2, 1, 1, 0, 2, 1, 1, 1, 0, 0, 5, 4, 1, 1, 5, 1, 3, 1, 1, 1, 0, 2, 2, 1, 0, 1, 2, 5, 2, 0, 5, 2, 1, 5, 0, 2, 1, 5, 1, 3, 5, 5, 2, 1, 2, 1, 5, 1, 5, 3, 1, 0, 3, 0, 0, 1, 1, 0, 0, 0, 1, 1, 2, 1, 1, 2, 1, 5, 1, 0, 0, 1, 5, 1, 2, 4, 0, 3, 0, 5, 1, 0, 0, 1, 0, 2, 0, 3, 1, 2, 1, 3, 2, 2, 1, 5, 4, 1, 5, 1, 0, 1, 5, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 5, 5, 5, 2, 0, 0, 0, 1, 0, 2, 5, 3, 5, 0, 1, 2, 3, 5, 0, 1, 1, 0, 1, 1, 2, 1, 2, 2, 2, 5, 5, 5, 2, 1, 3, 0, 0, 5, 1, 1, 1, 1, 0, 1, 1, 3, 1, 1, 2, 0, 3, 5, 2, 1, 4, 4, 1, 5, 1, 5, 1, 0, 1, 2, 0, 1, 2, 1, 1, 2, 2, 0, 0, 5, 1, 1, 1, 1, 1, 1, 5, 0, 0, 1, 1, 1, 1, 0, 3, 0, 0, 5, 2, 1, 3, 2, 1, 2, 1, 1, 1, 1, 3, 0, 2, 2, 1, 1, 0, 1, 5, 1, 0, 5, 5, 5, 1, 2, 0, 3, 1, 1, 1, 2, 1, 1, 1, 1, 5, 1, 3, 0, 0, 5, 2, 1, 2, 0]\n",
            "هندزفری \t\t: electronic-devices\n",
            "مبل \t\t: for-the-home\n",
            "کفش \t\t: personal\n",
            "گوشی الجی \t\t: electronic-devices\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsxqAwVvwOdD",
        "colab_type": "text"
      },
      "source": [
        "## 6) Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75Ar7r9pyRLq",
        "colab_type": "text"
      },
      "source": [
        "It's time to evaluate your model using predicted categories for validation data.\n",
        "\n",
        "You need to create a confusion matrix based on your prediction and the real labels. Then you can use this confusion matrix for calculation other measuring metrics. \n",
        "\n",
        "As this problem is a multi-class problem, the calculation formula is a little different from the general case. Read [this article](https://towardsdatascience.com/multi-class-metrics-made-simple-part-i-precision-and-recall-9250280bddc2) for more information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZ-19cA3wiqT",
        "colab_type": "text"
      },
      "source": [
        "### 6.1) Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KB5CCVKbw2Po",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "1fd0f131-dfc1-4b95-88f2-9ef1666ef31f"
      },
      "source": [
        "def confusion_matrix(y_true, y_pred):\n",
        "    m = []\n",
        "    for a in set(y_pred + y_true):\n",
        "        m.append([])\n",
        "        for b in set(y_pred + y_true):\n",
        "            m[-1].append(sum([1 for i in range(len(y_true)) if y_true[i] == b and y_pred[i] == a]))\n",
        "    return np.matrix(m)\n",
        "\n",
        "cm_valid = confusion_matrix(valid_y, yp)\n",
        "cm = confusion_matrix(y, y_p_train)\n",
        "print(cm)\n",
        "print('valid set:')\n",
        "print(cm_valid)"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 745    5    7    2    1    4]\n",
            " [  10 1034   10   11   56   19]\n",
            " [   5    4  587    3    6    3]\n",
            " [   1    1    0  324    1    0]\n",
            " [   1    5    2    1  151    2]\n",
            " [   5    8    0    2    8  476]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM6GgtHNwl1a",
        "colab_type": "text"
      },
      "source": [
        "### 6.2) Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hL4wy510lEw",
        "colab_type": "text"
      },
      "source": [
        "$$\\text{Accuracy} = \\frac{TP + TN}{TP + FP + FN + TN}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXPscPgkw4_S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "580ada4b-57a3-4864-8e2c-1ed0388e86e6"
      },
      "source": [
        "accuracy = lambda cm: sum([cm[i, i] for i in range(len(cm))]) / cm.sum()\n",
        "print('train set:', accuracy(cm))\n",
        "print('valid set:', accuracy(cm_valid))"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train set: 0.9477142857142857\n",
            "valid set: 0.816\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTFur0U5wsmO",
        "colab_type": "text"
      },
      "source": [
        "### 6.3) Precision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frrTkQWG09fd",
        "colab_type": "text"
      },
      "source": [
        "$$\\text{Precision} = \\frac{TP}{TP + FP}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1SGaY9qw7Z4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "f2055736-5e4f-4a09-8521-77b1a903a68b"
      },
      "source": [
        "print('train:')\n",
        "precisions = lambda cm: [cm[i, i] / int(cm.sum(1)[i]) for i in range(len(cm))]\n",
        "print(precisions(cm))\n",
        "precision = lambda cm: sum(precisions(cm)) / len(precisions(cm))\n",
        "print(precision(cm))\n",
        "\n",
        "print('valid:')\n",
        "print(precision(cm_valid))\n"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train:\n",
            "[0.975130890052356, 0.9070175438596492, 0.9654605263157895, 0.9908256880733946, 0.9320987654320988, 0.9539078156312625]\n",
            "0.954073538227425\n",
            "valid:\n",
            "0.8189172914568089\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQcpDrm1wuhU",
        "colab_type": "text"
      },
      "source": [
        "### 6.4) Recall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBb1eop61JlG",
        "colab_type": "text"
      },
      "source": [
        "$$\\text{Recall} = \\frac{TP}{TP + FN}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8bLKc6Ew_iR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "418f5b95-9d53-45b8-962f-5305188f8b8d"
      },
      "source": [
        "print('train:')\n",
        "recalls = lambda cm: [cm[i, i] / int(cm.sum(0)[0,i]) for i in range(len(cm))]\n",
        "recall = lambda cm: sum(recalls(cm)) / len(recalls(cm))\n",
        "print(recalls(cm))\n",
        "print(recall(cm))\n",
        "\n",
        "print('valid:')\n",
        "print(recall(cm_valid))"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train:\n",
            "[0.9713168187744459, 0.978240302743614, 0.9686468646864687, 0.9446064139941691, 0.6771300448430493, 0.9444444444444444]\n",
            "0.9140641482476987\n",
            "valid:\n",
            "0.7299482236968752\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWx42Dr5wx4m",
        "colab_type": "text"
      },
      "source": [
        "### 6.5) F1 score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdVnZfm21SCM",
        "colab_type": "text"
      },
      "source": [
        "$$\\text{F1 score} = 2\\times \\frac{(Recall \\times  Precision)}{Recall + Precision}$$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FkCog8ExBXA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "daa2ccbd-2fe7-46af-f6a5-29a7e40ee209"
      },
      "source": [
        "print('train:')\n",
        "f1_scores = lambda cm: [2 * (recalls(cm)[i] * precisions(cm)[i]) / (recalls(cm)[i] + precisions(cm)[i]) for i in range(len(cm))]\n",
        "print(f1_scores(cm))\n",
        "f1_score = lambda cm: sum(f1_scores(cm)) / len(f1_scores(cm))  # or :   2 * (recall * precision) / (recall + precision)\n",
        "print(f1_score(cm))\n",
        "\n",
        "print('valid:')\n",
        "print(f1_score(cm_valid))"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train:\n",
            "[0.9732201175702155, 0.9412835685025033, 0.9670510708401977, 0.9671641791044776, 0.7844155844155845, 0.9491525423728814]\n",
            "0.93038117713431\n",
            "valid:\n",
            "0.7615377325462703\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZP1GHbtB2T7",
        "colab_type": "text"
      },
      "source": [
        "## 7) K-Fold Cross Validation *(Optional)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayL96oOoB-aq",
        "colab_type": "text"
      },
      "source": [
        "Evaluate your model based on the K-Fold Cross Validation approach. This step is optional and has a few extra points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Zxs_90TC-3b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your implementation"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}